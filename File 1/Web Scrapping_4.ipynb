{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38b778e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c34548",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d14ecf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank: 1.\n",
      "Name: \"Baby Shark Dance\"[6]\n",
      "Artist: Pinkfong Baby Shark - Kids' Songs & Stories\n",
      "Upload Date: 13.48\n",
      "Views: June 17, 2016\n",
      "\n",
      "Rank: 2.\n",
      "Name: \"Despacito\"[9]\n",
      "Artist: Luis Fonsi\n",
      "Upload Date: 8.28\n",
      "Views: January 12, 2017\n",
      "\n",
      "Rank: 3.\n",
      "Name: \"Johny Johny Yes Papa\"[17]\n",
      "Artist: LooLoo Kids - Nursery Rhymes and Children's Songs\n",
      "Upload Date: 6.82\n",
      "Views: October 8, 2016\n",
      "\n",
      "Rank: 4.\n",
      "Name: \"Bath Song\"[18]\n",
      "Artist: Cocomelon - Nursery Rhymes\n",
      "Upload Date: 6.45\n",
      "Views: May 2, 2018\n",
      "\n",
      "Rank: 5.\n",
      "Name: \"Shape of You\"[19]\n",
      "Artist: Ed Sheeran\n",
      "Upload Date: 6.11\n",
      "Views: January 30, 2017\n",
      "\n",
      "Rank: 6.\n",
      "Name: \"See You Again\"[22]\n",
      "Artist: Wiz Khalifa\n",
      "Upload Date: 6.05\n",
      "Views: April 6, 2015\n",
      "\n",
      "Rank: 7.\n",
      "Name: \"Wheels on the Bus\"[27]\n",
      "Artist: Cocomelon - Nursery Rhymes\n",
      "Upload Date: 5.62\n",
      "Views: May 24, 2018\n",
      "\n",
      "Rank: 8.\n",
      "Name: \"Phonics Song with Two Words\"[28]\n",
      "Artist: ChuChu TV Nursery Rhymes & Kids Songs\n",
      "Upload Date: 5.52\n",
      "Views: March 6, 2014\n",
      "\n",
      "Rank: 9.\n",
      "Name: \"Uptown Funk\"[29]\n",
      "Artist: Mark Ronson\n",
      "Upload Date: 5.05\n",
      "Views: November 19, 2014\n",
      "\n",
      "Rank: 10.\n",
      "Name: \"Learning Colors – Colorful Eggs on a Farm\"[30]\n",
      "Artist: Miroshka TV\n",
      "Upload Date: 4.99\n",
      "Views: February 27, 2018\n",
      "\n",
      "Rank: 11.\n",
      "Name: \"Gangnam Style\"[31]\n",
      "Artist: officialpsy\n",
      "Upload Date: 4.92\n",
      "Views: July 15, 2012\n",
      "\n",
      "Rank: 12.\n",
      "Name: \"Masha and the Bear – Recipe for Disaster\"[36]\n",
      "Artist: Get Movies\n",
      "Upload Date: 4.56\n",
      "Views: January 31, 2012\n",
      "\n",
      "Rank: 13.\n",
      "Name: \"Dame Tu Cosita\"[37]\n",
      "Artist: Ultra Records\n",
      "Upload Date: 4.46\n",
      "Views: April 5, 2018\n",
      "\n",
      "Rank: 14.\n",
      "Name: \"Axel F\"[38]\n",
      "Artist: Crazy Frog\n",
      "Upload Date: 4.09\n",
      "Views: June 16, 2009\n",
      "\n",
      "Rank: 15.\n",
      "Name: \"Sugar\"[39]\n",
      "Artist: Maroon 5\n",
      "Upload Date: 3.95\n",
      "Views: January 14, 2015\n",
      "\n",
      "Rank: 16.\n",
      "Name: \"Counting Stars\"[40]\n",
      "Artist: OneRepublic\n",
      "Upload Date: 3.89\n",
      "Views: May 31, 2013\n",
      "\n",
      "Rank: 17.\n",
      "Name: \"Roar\"[41]\n",
      "Artist: Katy Perry\n",
      "Upload Date: 3.89\n",
      "Views: September 5, 2013\n",
      "\n",
      "Rank: 18.\n",
      "Name: \"Baa Baa Black Sheep\"[42]\n",
      "Artist: Cocomelon - Nursery Rhymes\n",
      "Upload Date: 3.80\n",
      "Views: June 25, 2018\n",
      "\n",
      "Rank: 19.\n",
      "Name: \"Waka Waka (This Time for Africa)\"[43]\n",
      "Artist: Shakira\n",
      "Upload Date: 3.75\n",
      "Views: June 4, 2010\n",
      "\n",
      "Rank: 20.\n",
      "Name: \"Sorry\"[44]\n",
      "Artist: Justin Bieber\n",
      "Upload Date: 3.72\n",
      "Views: October 22, 2015\n",
      "\n",
      "Rank: 21.\n",
      "Name: \"Lakdi Ki Kathi\"[45]\n",
      "Artist: Jingle Toons\n",
      "Upload Date: 3.71\n",
      "Views: June 14, 2018\n",
      "\n",
      "Rank: 22.\n",
      "Name: \"Thinking Out Loud\"[46]\n",
      "Artist: Ed Sheeran\n",
      "Upload Date: 3.67\n",
      "Views: October 7, 2014\n",
      "\n",
      "Rank: 23.\n",
      "Name: \"Dark Horse\"[47]\n",
      "Artist: Katy Perry\n",
      "Upload Date: 3.60\n",
      "Views: February 20, 2014\n",
      "\n",
      "Rank: 24.\n",
      "Name: \"Humpty the train on a fruits ride\"[48]\n",
      "Artist: Kiddiestv Hindi - Nursery Rhymes & Kids Songs\n",
      "Upload Date: 3.58\n",
      "Views: January 26, 2018\n",
      "\n",
      "Rank: 25.\n",
      "Name: \"Perfect\"[49]\n",
      "Artist: Ed Sheeran\n",
      "Upload Date: 3.56\n",
      "Views: November 9, 2017\n",
      "\n",
      "Rank: 26.\n",
      "Name: \"Let Her Go\"[50]\n",
      "Artist: Passenger\n",
      "Upload Date: 3.53\n",
      "Views: July 25, 2012\n",
      "\n",
      "Rank: 27.\n",
      "Name: \"Faded\"[51]\n",
      "Artist: Alan Walker\n",
      "Upload Date: 3.53\n",
      "Views: December 3, 2015\n",
      "\n",
      "Rank: 28.\n",
      "Name: \"Girls Like You\"[52]\n",
      "Artist: Maroon 5\n",
      "Upload Date: 3.50\n",
      "Views: May 31, 2018\n",
      "\n",
      "Rank: 29.\n",
      "Name: \"Shree Hanuman Chalisa\"[53]\n",
      "Artist: T-Series Bhakti Sagar\n",
      "Upload Date: 3.48\n",
      "Views: May 10, 2011\n",
      "\n",
      "Rank: 30.\n",
      "Name: \"Lean On\"[54]\n",
      "Artist: Major Lazer Official\n",
      "Upload Date: 3.48\n",
      "Views: March 22, 2015\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the URL\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\"\n",
    "\n",
    "# Send request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# HTML content of the page using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find the table that contains the information \n",
    "table = soup.find('table', {'class': 'wikitable'})\n",
    "\n",
    "# Initialize empty lists to store the details\n",
    "rank_list = []\n",
    "name_list = []\n",
    "artist_list = []\n",
    "upload_date_list = []\n",
    "views_list = []\n",
    "\n",
    "# Iterate through the rows of the table\n",
    "for row in table.find_all('tr')[1:]:  \n",
    "    columns = row.find_all('td')\n",
    "    if len(columns) >= 5:\n",
    "        rank = columns[0].text.strip()\n",
    "        name = columns[1].text.strip()\n",
    "        artist = columns[2].text.strip()\n",
    "        upload_date = columns[3].text.strip()\n",
    "        views = columns[4].text.strip()\n",
    "\n",
    "        rank_list.append(rank)\n",
    "        name_list.append(name)\n",
    "        artist_list.append(artist)\n",
    "        upload_date_list.append(upload_date)\n",
    "        views_list.append(views)\n",
    "\n",
    "# Print the data\n",
    "for i in range(len(rank_list)):\n",
    "    print(f\"Rank: {rank_list[i]}\")\n",
    "    print(f\"Name: {name_list[i]}\")\n",
    "    print(f\"Artist: {artist_list[i]}\")\n",
    "    print(f\"Upload Date: {upload_date_list[i]}\")\n",
    "    print(f\"Views: {views_list[i]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "495a1782",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bff6c0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixtures link not found on the BCCI website.\n"
     ]
    }
   ],
   "source": [
    "# Define the URL of the BCCI website\n",
    "url = \"https://www.bcci.tv/\"\n",
    "\n",
    "# Send a GET request to the BCCI website\n",
    "response = requests.get(url)\n",
    "\n",
    "#  HTML content of the page using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find the link to the international fixtures page\n",
    "fixtures_link = soup.find('a', text=\"Fixtures\")\n",
    "\n",
    "if fixtures_link:\n",
    "    fixtures_url = fixtures_link['href']\n",
    "\n",
    "    # Construct the full URL for the fixtures page\n",
    "    full_fixtures_url = f\"https://www.bcci.tv{fixtures_url}\"\n",
    "\n",
    "    # Send a GET request to the fixtures page\n",
    "    fixtures_response = requests.get(full_fixtures_url)\n",
    "\n",
    "    # HTML content of the fixtures page using BeautifulSoup\n",
    "    fixtures_soup = BeautifulSoup(fixtures_response.content, 'html.parser')\n",
    "\n",
    "    # Find and scrape the details of international fixtures\n",
    "    for fixture in fixtures_soup.find_all('div', class_='js-list'):\n",
    "        series = fixture.find('strong').text\n",
    "        place = fixture.find('span', class_='fixture__info__location').text\n",
    "        date = fixture.find('span', class_='fixture__datetime').text\n",
    "        time = fixture.find('span', class_='fixture__datetime__local').text\n",
    "\n",
    "        print(f\"Series: {series}\")\n",
    "        print(f\"Place: {place}\")\n",
    "        print(f\"Date: {date}\")\n",
    "        print(f\"Time: {time}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"Fixtures link not found on the BCCI website.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b497566b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a485941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indian Economy link not found on the StatisticsTimes website.\n"
     ]
    }
   ],
   "source": [
    "# Define the URL of the StatisticsTimes website\n",
    "url = \"http://statisticstimes.com/\"\n",
    "\n",
    "# Send a GET request to the StatisticsTimes website\n",
    "response = requests.get(url)\n",
    "\n",
    "#  HTML content of the page using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find the link to the Indian Economy page\n",
    "economy_link = soup.find('a', text=\"Indian Economy\")\n",
    "\n",
    "if economy_link:\n",
    "    economy_url = economy_link['href']\n",
    "\n",
    "    # Construct the full URL for the Indian Economy page\n",
    "    full_economy_url = f\"http://statisticstimes.com{economy_url}\"\n",
    "\n",
    "    # Send a GET request to the Indian Economy page\n",
    "    economy_response = requests.get(full_economy_url)\n",
    "\n",
    "    # Parse the HTML content of the Indian Economy page using BeautifulSoup\n",
    "    economy_soup = BeautifulSoup(economy_response.content, 'html.parser')\n",
    "\n",
    "    # Find and scrape the details of state-wise GDP\n",
    "    table = economy_soup.find('table', {'class': 'display nowrap'})\n",
    "\n",
    "    if table:\n",
    "        rows = table.find_all('tr')[1:]  \n",
    "        for row in rows:\n",
    "            columns = row.find_all('td')\n",
    "            rank = columns[0].text.strip()\n",
    "            state = columns[1].text.strip()\n",
    "            gdp_18_19 = columns[2].text.strip()\n",
    "            gdp_19_20 = columns[3].text.strip()\n",
    "            share_18_19 = columns[4].text.strip()\n",
    "            gdp_billion = columns[5].text.strip()\n",
    "\n",
    "            print(f\"Rank: {rank}\")\n",
    "            print(f\"State: {state}\")\n",
    "            print(f\"GSDP(18-19) at current prices: {gdp_18_19}\")\n",
    "            print(f\"GSDP(19-20) at current prices: {gdp_19_20}\")\n",
    "            print(f\"Share(18-19): {share_18_19}\")\n",
    "            print(f\"GDP($ billion): {gdp_billion}\")\n",
    "            print()\n",
    "\n",
    "    else:\n",
    "        print(\"Table not found on the Indian Economy page.\")\n",
    "else:\n",
    "    print(\"Indian Economy link not found on the StatisticsTimes website.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61987027",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9aaf1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explore menu not found on the GitHub website.\n"
     ]
    }
   ],
   "source": [
    "# Define the URL of the GitHub website\n",
    "url = \"https://github.com/\"\n",
    "\n",
    "# Send a GET request to the GitHub website\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the page using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find the Explore menu and click on the Trending option\n",
    "explore_menu = soup.find('summary', text=\"Explore\")\n",
    "\n",
    "if explore_menu:\n",
    "    trending_link = explore_menu.find_next('a', text=\"Trending\")\n",
    "    if trending_link:\n",
    "        trending_url = trending_link['href']\n",
    "\n",
    "        # Construct the full URL for the trending page\n",
    "        full_trending_url = f\"https://github.com{trending_url}\"\n",
    "\n",
    "        # Send a GET request to the trending page\n",
    "        trending_response = requests.get(full_trending_url)\n",
    "\n",
    "        # Parse the HTML content of the trending page using BeautifulSoup\n",
    "        trending_soup = BeautifulSoup(trending_response.content, 'html.parser')\n",
    "\n",
    "        # Find and scrape the details of trending repositories\n",
    "        for repo in trending_soup.find_all('article', class_='Box-row'):\n",
    "            title = repo.find('h1').text.strip()\n",
    "            description = repo.find('p', class_='col-9').text.strip()\n",
    "            contributors = repo.find('a', class_='muted-link').text.strip()\n",
    "            language = repo.find('span', itemprop='programmingLanguage').text.strip()\n",
    "\n",
    "            print(f\"Repository Title: {title}\")\n",
    "            print(f\"Repository Description: {description}\")\n",
    "            print(f\"Contributors Count: {contributors}\")\n",
    "            print(f\"Language Used: {language}\")\n",
    "            print()\n",
    "\n",
    "    else:\n",
    "        print(\"Trending link not found on the GitHub website.\")\n",
    "else:\n",
    "    print(\"Explore menu not found on the GitHub website.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cd637a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f04042ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explore menu not found on the Billboard website.\n"
     ]
    }
   ],
   "source": [
    "# Define the URL of the Billboard website\n",
    "url = \"https://www.billboard.com/\"\n",
    "\n",
    "# Send a GET request to the Billboard website\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the page using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find the Explore menu and click on the Hot 100 page link\n",
    "explore_menu = soup.find('li', class_='site-header__menu-item--explore')\n",
    "\n",
    "if explore_menu:\n",
    "    hot_100_link = explore_menu.find('a', text=\"Hot 100\")\n",
    "\n",
    "    if hot_100_link:\n",
    "        hot_100_url = hot_100_link['href']\n",
    "\n",
    "        # Construct the full URL for the Hot 100 page\n",
    "        full_hot_100_url = f\"https://www.billboard.com{hot_100_url}\"\n",
    "\n",
    "        # Send a GET request to the Hot 100 page\n",
    "        hot_100_response = requests.get(full_hot_100_url)\n",
    "\n",
    "        # Parse the HTML content of the Hot 100 page using BeautifulSoup\n",
    "        hot_100_soup = BeautifulSoup(hot_100_response.content, 'html.parser')\n",
    "\n",
    "        # Find and scrape the details of the top 100 songs\n",
    "        songs = hot_100_soup.find_all('li', class_='chart-list__element')\n",
    "\n",
    "        for song in songs:\n",
    "            song_name = song.find('span', class_='chart-list-item__title-text').text.strip()\n",
    "            artist_name = song.find('span', class_='chart-list-item__artist').text.strip()\n",
    "            last_week_rank = song.find('div', class_='chart-list-item__last-week').text.strip()\n",
    "            peak_rank = song.find('div', class_='chart-list-item__weeks-at-one').text.strip()\n",
    "            weeks_on_board = song.find('div', class_='chart-list-item__weeks-on-chart').text.strip()\n",
    "\n",
    "            print(f\"Song Name: {song_name}\")\n",
    "            print(f\"Artist Name: {artist_name}\")\n",
    "            print(f\"Last Week Rank: {last_week_rank}\")\n",
    "            print(f\"Peak Rank: {peak_rank}\")\n",
    "            print(f\"Weeks on Board: {weeks_on_board}\")\n",
    "            print()\n",
    "\n",
    "    else:\n",
    "        print(\"Hot 100 page link not found on the Billboard website.\")\n",
    "else:\n",
    "    print(\"Explore menu not found on the Billboard website.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23ebb0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "301b1fa8",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18808/1393973476.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'td'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mbook_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mauthor_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mvolumes_sold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mpublisher\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the URL of The Guardian's webpage\n",
    "url = \"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare\"\n",
    "\n",
    "# Send a GET request to the webpage\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the page using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find and scrape the details of the highest selling novels\n",
    "table = soup.find('table', {'class': 'in-article sortable'})\n",
    "\n",
    "if table:\n",
    "    rows = table.find_all('tr')[1:]  # Skip the header row\n",
    "    for row in rows:\n",
    "        columns = row.find_all('td')\n",
    "        book_name = columns[0].text.strip()\n",
    "        author_name = columns[1].text.strip()\n",
    "        volumes_sold = columns[2].text.strip()\n",
    "        publisher = columns[3].text.strip()\n",
    "        genre = columns[4].text.strip()\n",
    "\n",
    "        print(f\"Book Name: {book_name}\")\n",
    "        print(f\"Author Name: {author_name}\")\n",
    "        print(f\"Volumes Sold: {volumes_sold}\")\n",
    "        print(f\"Publisher: {publisher}\")\n",
    "        print(f\"Genre: {genre}\")\n",
    "        print()\n",
    "\n",
    "else:\n",
    "    print(\"Table not found on the webpage. Please check the webpage's structure.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3d1b3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb12ebb0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18808/637880095.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mgenre\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mseries\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'span'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'genre'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mrun_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mseries\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'span'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'runtime'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mratings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mseries\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'strong'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[0mvotes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mseries\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'span'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'nv'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "# Define the URL of the IMDb webpage\n",
    "url = \"https://www.imdb.com/list/ls095964455/\"\n",
    "\n",
    "# Send a GET request to the IMDb webpage\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the page using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find and scrape the details of the most watched TV series\n",
    "series_list = soup.find_all('div', class_='lister-item-content')\n",
    "\n",
    "for series in series_list:\n",
    "    name = series.find('h3', class_='lister-item-header').a.text.strip()\n",
    "    year_span = series.find('span', class_='lister-item-year').text.strip()\n",
    "    genre = series.find('span', class_='genre').text.strip()\n",
    "    run_time = series.find('span', class_='runtime').text.strip()\n",
    "    ratings = series.find('strong').text.strip()\n",
    "    votes = series.find('span', {'name': 'nv'}).text.strip().replace(',', '')\n",
    "\n",
    "    print(f\"Name: {name}\")\n",
    "    print(f\"Year Span: {year_span}\")\n",
    "    print(f\"Genre: {genre}\")\n",
    "    print(f\"Run Time: {run_time}\")\n",
    "    print(f\"Ratings: {ratings}\")\n",
    "    print(f\"Votes: {votes}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7d70ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c47b180e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View ALL Data Sets link not found on the UCI Machine Learning Repository website.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the URL of the UCI Machine Learning Repository website\n",
    "url = \"https://archive.ics.uci.edu/\"\n",
    "\n",
    "# Send a GET request to the website\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the page using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find and click the \"View ALL Data Sets\" link\n",
    "all_datasets_link = soup.find('a', href='ml/datasets.php')\n",
    "\n",
    "if all_datasets_link:\n",
    "    all_datasets_url = f\"https://archive.ics.uci.edu/ml/{all_datasets_link['href']}\"\n",
    "\n",
    "    # Send a GET request to the \"View ALL Data Sets\" page\n",
    "    datasets_response = requests.get(all_datasets_url)\n",
    "\n",
    "    # Parse the HTML content of the \"View ALL Data Sets\" page using BeautifulSoup\n",
    "    datasets_soup = BeautifulSoup(datasets_response.content, 'html.parser')\n",
    "\n",
    "    # Find and scrape the details of datasets\n",
    "    datasets_table = datasets_soup.find('table', {'border': \"1\"})\n",
    "\n",
    "    if datasets_table:\n",
    "        rows = datasets_table.find_all('tr')[1:]  # Skip the header row\n",
    "        for row in rows:\n",
    "            columns = row.find_all('td')\n",
    "            dataset_name = columns[0].text.strip()\n",
    "            data_type = columns[1].text.strip()\n",
    "            task = columns[2].text.strip()\n",
    "            attribute_type = columns[3].text.strip()\n",
    "            no_of_instances = columns[4].text.strip()\n",
    "            no_of_attributes = columns[5].text.strip()\n",
    "            year = columns[6].text.strip()\n",
    "\n",
    "            print(f\"Dataset Name: {dataset_name}\")\n",
    "            print(f\"Data Type: {data_type}\")\n",
    "            print(f\"Task: {task}\")\n",
    "            print(f\"Attribute Type: {attribute_type}\")\n",
    "            print(f\"No of Instances: {no_of_instances}\")\n",
    "            print(f\"No of Attributes: {no_of_attributes}\")\n",
    "            print(f\"Year: {year}\")\n",
    "            print()\n",
    "\n",
    "    else:\n",
    "        print(\"Table not found on the 'View ALL Data Sets' page.\")\n",
    "else:\n",
    "    print(\"View ALL Data Sets link not found on the UCI Machine Learning Repository website.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
